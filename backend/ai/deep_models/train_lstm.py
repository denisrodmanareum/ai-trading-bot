"""
LSTM Training Script
"""
import asyncio
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
from datetime import datetime
from loguru import logger
import os
import sys

# Add parent directory to path to import ai modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from ai.deep_models.lstm_predictor import LSTMPredictor, DeepLearningPredictor
from ai.trainer import fetch_training_data
from app.core.config import settings

async def train_lstm(
    symbol: str = "BTCUSDT",
    interval: str = "1h",
    days: int = 30,
    epochs: int = 50,
    batch_size: int = 32,
    sequence_length: int = 100,
    learning_rate: float = 0.001
):
    """LSTM ëª¨ë¸ í•™ìŠµ ë° ì €ì¥"""
    logger.info(f"ğŸš€ Starting LSTM training for {symbol} ({interval})...")
    
    # 1. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    df = await fetch_training_data(symbol, interval, days)
    
    if len(df) < sequence_length + 10:
        logger.error(f"Not enough data for training: {len(df)} candles")
        return
    
    # 2. ë°ì´í„° ì¤€ë¹„
    predictor = DeepLearningPredictor(model_type='lstm')
    
    # í”¼ì²˜ ì»¬ëŸ¼ ì •ì˜ (features.py ì°¸ê³ )
    feature_cols = [
        'close', 'volume', 'rsi', 'macd', 'signal',
        'bb_upper', 'bb_middle', 'bb_lower', 'atr',
        'stoch_k', 'stoch_d', 'ema_9', 'ema_21', 'ema_50',
        'returns', 'log_returns', 'volume_ratio', 'high_low_ratio',
        'candle_body', 'upper_shadow'
    ]
    
    # ê°€ìš© í”¼ì²˜ë§Œ ì„ íƒ
    available_features = [col for col in feature_cols if col in df.columns]
    data = df[available_features].values
    
    # ì •ê·œí™”
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0) + 1e-8
    data_norm = (data - mean) / std
    
    # íƒ€ê²Ÿ ìƒì„± (ë‹¤ìŒ ìº”ë“¤ì˜ ìˆ˜ìµë¥ )
    # predictor.pyì˜ fc2 ì¶œë ¥ì´ 3ê°œ [price_change, up_prob, down_prob] ì„ì„ ê³ ë ¤
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ë‹¤ìŒ ìº”ë“¤ì˜ ë³€í™”ëŸ‰ê³¼ ë°©í–¥ì„ í•™ìŠµ
    X, y = [], []
    for i in range(len(data_norm) - sequence_length):
        X.append(data_norm[i:i+sequence_length])
        
        # ë‹¤ìŒ ìº”ë“¤ì˜ ë¡œê·¸ ìˆ˜ìµë¥ 
        next_return = df['log_returns'].iloc[i+sequence_length]
        
        # [change, up, down] í˜•ì‹ì˜ íƒ€ê²Ÿ
        # up/downì€ ì„ê³„ê°’ 0.001 (0.1%) ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
        up = 1.0 if next_return > 0.001 else 0.0
        down = 1.0 if next_return < -0.001 else 0.0
        y.append([next_return * 100, up, down]) # ìˆ˜ìµë¥ ì€ % ë‹¨ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§
    
    X = torch.FloatTensor(np.array(X)).to(predictor.device)
    y = torch.FloatTensor(np.array(y)).to(predictor.device)
    
    dataset = TensorDataset(X, y)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = predictor.model
    criterion = nn.MSELoss() # ê°„ë‹¨í•˜ê²Œ MSE ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë³µí•© ì†ì‹¤ í•¨ìˆ˜ ê³ ë ¤ ê°€ëŠ¥)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    
    # 4. í•™ìŠµ ë£¨í”„
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0
        for batch_X, batch_y in loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        
        if (epoch + 1) % 10 == 0:
            logger.info(f"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(loader):.6f}")
    
    # 5. ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    model_name = f"lstm_{symbol}_{interval}_{timestamp}.pt"
    save_path = os.path.join(settings.AI_MODEL_PATH, model_name)
    
    os.makedirs(settings.AI_MODEL_PATH, exist_ok=True)
    predictor.save_model(save_path)
    
    # ê¸°ì¡´ ê°™ì€ interval ëª¨ë¸ ì‚­ì œ (ìµœì‹  í•˜ë‚˜ë§Œ ìœ ì§€)
    import glob
    pattern = os.path.join(settings.AI_MODEL_PATH, f"lstm_{symbol}_{interval}_*.pt")
    existing = sorted(glob.glob(pattern), reverse=True)
    for old in existing[1:]:
        try:
            os.remove(old)
            logger.info(f"ğŸ—‘ï¸ Cleaned up old LSTM model: {os.path.basename(old)}")
        except:
            pass
            
    logger.info(f"âœ… LSTM training finished. Model saved: {model_name}")
    return save_path

if __name__ == "__main__":
    # ìˆœì°¨ì  í•™ìŠµ ì‹¤í–‰
    async def run_all():
        # 1ì‹œê°„ë´‰ í•™ìŠµ
        await train_lstm(interval="1h", days=30, epochs=30)
        # 1ë¶„ë´‰ í•™ìŠµ
        await train_lstm(interval="1m", days=3, epochs=30) # 1ë¶„ë´‰ì€ ë°ì´í„°ê°€ ë§ìœ¼ë¯€ë¡œ ë‚ ì§œ ì¶•ì†Œ
        
    asyncio.run(run_all())
